{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-To-Text LLM Server\n",
    "\n",
    "**important: Select venv Python Interpreter before you start**\n",
    "\n",
    "This repository is designed to be used with Visual Studio Code and Docker DevContainer.\n",
    "\n",
    "![dev-container](../img/dev-container.png)\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "a) Download model\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/Mistral-7B-Instruct-v0.3-exl2 \\\n",
    "    --revision 1a09a351a5fb5a356102bfca2d26507cdab11111 \\\n",
    "    --local-dir ~/.gai/models/exllamav2-mistral7b \\\n",
    "    --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/dolphin-2.9-llama3-8b-exl2 \\\n",
    "    --revision 6521bd8b0f793a038f85d445316c94cdd0957d8e \\\n",
    "    --branch 4_25 \\\n",
    "    --local-dir ~/.gai/models/exllamav2-mistral7b \\\n",
    "    --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "b) Create gai.yml in ~/.gai\n",
    "\n",
    "```yaml\n",
    "generators:\n",
    "    ttt:\n",
    "        default: \"ttt-exllamav2-dolphin\"\n",
    "        configs:\n",
    "            ttt-exllamav2-dolphin:\n",
    "                type: \"ttt\"\n",
    "                engine: \"exllamav2\"\n",
    "                model: \"dolphin\"\n",
    "                name: \"ttt-exllamav2-dolphin\"\n",
    "                model_path: \"models/exllamav2-dolphin\"\n",
    "                model_basename: \"model\"\n",
    "                max_seq_len: 8192\n",
    "                prompt_format: \"mistral\"\n",
    "                hyperparameters:\n",
    "                    temperature: 0.85\n",
    "                    top_p: 0.8\n",
    "                    top_k: 50\n",
    "                    max_tokens: 1000\n",
    "                    tool_choice: \"auto\"\n",
    "                    max_retries: 5\n",
    "                    stop: [\"<|im_end|>\", \"</s>\", \"[/INST]\"]\n",
    "                extra:                    \n",
    "                    no_flash_attn: true\n",
    "                    seed: null\n",
    "                    decode_special_tokens: false\n",
    "                module:\n",
    "                    name: \"gai.ttt.server.gai_exllamav2\"\n",
    "                    class: \"GaiExLlamav2\"\n",
    "            ttt-exllamav2-mistral7b:\n",
    "                type: \"ttt\"\n",
    "                engine: \"exllamav2\"\n",
    "                model: \"mistral7b\"\n",
    "                name: \"ttt-exllamav2-mistral7b\"\n",
    "                model_path: \"models/exllamav2-mistral7b\"\n",
    "                model_basename: \"model\"\n",
    "                max_seq_len: 8192\n",
    "                prompt_format: \"mistral\"\n",
    "                hyperparameters:\n",
    "                    temperature: 0.85\n",
    "                    top_p: 0.8\n",
    "                    top_k: 50\n",
    "                    max_tokens: 1000\n",
    "                    tool_choice: \"auto\"\n",
    "                    max_retries: 5\n",
    "                    stop: [\"<|im_end|>\", \"</s>\", \"[/INST]\"]\n",
    "                extra:\n",
    "                    no_flash_attn: true\n",
    "                    seed: null\n",
    "                    decode_special_tokens: false\n",
    "                module:\n",
    "                    name: \"gai.ttt.server.gai_exllamav2\"\n",
    "                    class: \"GaiExLlamav2\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"app_dir\":\"/home/kakkoii1337/.gai\"}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">6.05</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m6.05\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Performance load_config                      </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Metric            </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Initial Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Change Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">  Final Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Function Duration </span>│<span style=\"color: #008000; text-decoration-color: #008000\">             - </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 4.91 seconds </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 4.91 seconds </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU Memory        </span>│<span style=\"color: #008000; text-decoration-color: #008000\">          - MB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">     21.16 MB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     21.16 MB </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CUDA Memory       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       6.04 GB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">     -0.03 GB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      6.07 GB </span>│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Performance load_config                      \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mMetric           \u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mInitial Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mChange Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129m Final Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mFunction Duration\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m            -\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m4.91 seconds\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m4.91 seconds\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU Memory       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m         - MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m    21.16 MB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    21.16 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCUDA Memory      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      6.04 GB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m    -0.03 GB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     6.07 GB\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                      Performance load_model                       </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Metric            </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Initial Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Change Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">  Final Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Function Duration </span>│<span style=\"color: #008000; text-decoration-color: #008000\">             - </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 0.03 seconds </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 0.03 seconds </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU Memory        </span>│<span style=\"color: #008000; text-decoration-color: #008000\">          - MB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">      0.22 MB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      0.22 MB </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CUDA Memory       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       6.07 GB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">      0.00 GB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      6.06 GB </span>│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                      Performance load_model                       \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mMetric           \u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mInitial Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mChange Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129m Final Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mFunction Duration\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m            -\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m0.03 seconds\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m0.03 seconds\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU Memory       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m         - MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m     0.22 MB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     0.22 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCUDA Memory      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      6.07 GB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m     0.00 GB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     6.06 GB\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                       Performance load_cache                        </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Metric            </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Initial Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">  Change Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">   Final Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Function Duration </span>│<span style=\"color: #008000; text-decoration-color: #008000\">             - </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 22.48 seconds </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 22.48 seconds </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU Memory        </span>│<span style=\"color: #008000; text-decoration-color: #008000\">          - MB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">      24.20 MB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      24.20 MB </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CUDA Memory       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       6.06 GB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">       4.71 GB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">       1.35 GB </span>│\n",
       "└───────────────────┴───────────────┴───────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                       Performance load_cache                        \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mMetric           \u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mInitial Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129m Change Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129m  Final Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mFunction Duration\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m            -\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m22.48 seconds\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m22.48 seconds\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU Memory       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m         - MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m     24.20 MB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     24.20 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCUDA Memory      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      6.06 GB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m      4.71 GB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m      1.35 GB\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────┴───────────────┴───────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                    Performance load_tokenizer                     </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Metric            </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Initial Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\"> Change Value </span>┃<span style=\"color: #af00ff; text-decoration-color: #af00ff; font-weight: bold\">  Final Value </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Function Duration </span>│<span style=\"color: #008000; text-decoration-color: #008000\">             - </span>│<span style=\"color: #808000; text-decoration-color: #808000\"> 2.69 seconds </span>│<span style=\"color: #800080; text-decoration-color: #800080\"> 2.69 seconds </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CPU Memory        </span>│<span style=\"color: #008000; text-decoration-color: #008000\">          - MB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">     49.66 MB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     49.66 MB </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> CUDA Memory       </span>│<span style=\"color: #008000; text-decoration-color: #008000\">       1.35 GB </span>│<span style=\"color: #808000; text-decoration-color: #808000\">      0.01 GB </span>│<span style=\"color: #800080; text-decoration-color: #800080\">      1.34 GB </span>│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                    Performance load_tokenizer                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mMetric           \u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mInitial Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129mChange Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\u001b[1;38;5;129m \u001b[0m\u001b[1;38;5;129m Final Value\u001b[0m\u001b[1;38;5;129m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36mFunction Duration\u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m            -\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m2.69 seconds\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m2.69 seconds\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCPU Memory       \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m         - MB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m    49.66 MB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    49.66 MB\u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mCUDA Memory      \u001b[0m\u001b[36m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m      1.35 GB\u001b[0m\u001b[32m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m     0.01 GB\u001b[0m\u001b[33m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m     1.34 GB\u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────┴───────────────┴──────────────┴──────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.34</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;91m1.34\u001b[0m\u001b[91m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5.85</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m5.85\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check .gairc\n",
    "import os\n",
    "gairc=None\n",
    "with open(os.path.expanduser(\"~/.gairc\"),\"r\") as f:\n",
    "    gairc = f.read()\n",
    "print(gairc)\n",
    "\n",
    "# check ~/.gairc (if docker created .gairc)\n",
    "import json\n",
    "jsoned=json.loads(gairc)\n",
    "assert os.path.expanduser(jsoned[\"app_dir\"])==\"/home/kakkoii1337/.gai\"\n",
    "\n",
    "# check ~/.gai (if docker created the mount point)\n",
    "assert os.path.exists(os.path.expanduser(\"~/.gai\"))\n",
    "\n",
    "# Initiate\n",
    "from gai.lib.server.singleton_host import SingletonHost\n",
    "from gai.lib.common.utils import free_mem\n",
    "from rich.console import Console\n",
    "console=Console()\n",
    "\n",
    "from gai.ttt.server.config.pydantic.ttt_config import TTTConfig\n",
    "ttt_config = TTTConfig(\n",
    "    type=\"ttt\",\n",
    "    engine=\"exllamav2\",\n",
    "    model=\"dolphin\",\n",
    "    name=\"ttt-exllamav2-dolphin\",\n",
    "    model_path=\"models/exllamav2-dolphin\",\n",
    "    max_seq_len=8192,\n",
    "    prompt_format=\"mistral\",\n",
    "    hyperparameters={\n",
    "        \"temperature\": 0.85,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 50,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"max_retries\": 5,\n",
    "        \"stop\": [\"<|im_end|>\", \"</s>\", \"[/INST]\"],\n",
    "    },\n",
    "    extra={\n",
    "        \"no_flash_attn\": True,\n",
    "        \"seed\": None,\n",
    "        \"decode_special_tokens\": False        \n",
    "    },\n",
    "    module={\n",
    "        \"name\": \"gai.ttt.server.gai_exllamav2\",\n",
    "        \"class\": \"GaiExLlamav2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# before loading\n",
    "free_mem()\n",
    "try:\n",
    "    with SingletonHost.GetInstanceFromConfig(ttt_config) as host:\n",
    "\n",
    "        # after loading\n",
    "        free_mem()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    # after disposal\n",
    "    free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Integration Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.32</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;91m1.32\u001b[0m\u001b[91m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.3221702575683594"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host = SingletonHost.GetInstanceFromConfig(ttt_config, verbose=False)\n",
    "host.load()\n",
    "generator = host.generator\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Test streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young boy named Timmy found a magical coin while playing in the park. As soon as he picked it up, he noticed that it had the power to grant wishes. Overwhelmed with excitement, he wished for a giant ice cream sundae. To his delight, a huge sundae appeared in front of him. He took a bite and it was the most delicious thing he had ever tasted. He continued to make wishes, each one more extravagant than the last, until the coin ran out of magic. As he sat there, a sad realization dawned on him: with all his wishes fulfilled, he was left with nothing but an empty sundae bowl and a longing for more."
     ]
    }
   ],
   "source": [
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=True)\n",
    "for message in response:\n",
    "    if message.choices[0].delta.content:\n",
    "        print(message.choices[0].delta.content, end=\"\", flush=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young boy named Tim, who lived in a small village, found a strange old key one day while playing near the river. Intrigued, he decided to use it to explore an abandoned castle on the hill. Inside, he discovered a hidden room filled with treasure and an ancient scroll. The scroll revealed that Tim was the lost heir to the castle and its riches. With this newfound knowledge, Tim used his wealth to help his village prosper, turning it into a thriving community. His life became a tale of adventure, wisdom, and generosity, inspiring everyone who heard his story.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-fb1da187-d8fe-4549-98ba-15bf9d92cab9', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_b25141a3-fbe5-41dd-b98a-da2f4c0b02f8', function=Function(arguments='{\"search_query\": \"current time in Singapore\"}', name='google'), type='function')]))], created=1734485527, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=50, prompt_tokens=418, total_tokens=468, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":\"What is the current time in Singapore?\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"\"}\n",
    "]\n",
    "tool_choice=\"required\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "response = host.generator.create(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=tool_choice,\n",
    "    stream=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4556/2228101676.py:18: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  json_schema=Book.schema(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-12d9675f-d4fe-4af8-84f9-9f4d76e2a5bd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\\n   \"title\": \"Foundation\",\\n   \"summary\": \"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\",\\n   \"author\": \"Isaac Asimov\",\\n   \"published_year\": 1951\\n }', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734485536, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=148, prompt_tokens=253, total_tokens=401, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Define Schema\n",
    "from pydantic import BaseModel\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = host.generator.create(messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
    "    json_schema=Book.schema(),\n",
    "    stream=False\n",
    "    )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.53</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;91m1.53\u001b[0m\u001b[91m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.5302543640136719"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del host.generator.model\n",
    "del host.generator.cache\n",
    "del host.generator.tokenizer\n",
    "del host.generator\n",
    "import gc,torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. API Test\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "a) Open Debug Icon and select **Python Debugger: gai-ttt server (dolphin)**\n",
    "\n",
    "b) Press `F5` to start the API server.\n",
    "\n",
    "c) Wait for the server to start.\n",
    "\n",
    "\n",
    "**Tests**:\n",
    "\n",
    "Run the following cells to test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Test Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-b35983a8-e65f-4379-89a8-ed1fa7711711\",\"choices\":[{\"finish_reason\":\"length\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"Once upon a time, in a small village nestled between the mountains and the sea, there lived a young woman named Amara. She was known throughout the village for her exceptional talent as a healer. Her knowledge of herbs and natural remedies was unparalleled, and she was often sought out by neighboring villages for her wisdom.\\n\\n One day, a traveler from a distant land arrived in the village. He was a tall man with a long, flowing beard,\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1734485610,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":100,\"prompt_tokens\":21,\"total_tokens\":121,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"max_tokens\\\":100,\\\n",
    "        \\\"tool_choice\\\": \\\"none\\\"}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young boy named Timmy found a magical coin while playing in the park. He rubbed it and wished for a pet dragon, and suddenly, a baby dragon appeared before him. They became the best of friends, going on many adventures together, and Tim"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "import asyncio\n",
    "from openai import ChatCompletion\n",
    "\n",
    "json_payload = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_tokens\": 50,\n",
    "    \"stream\": \"true\",  # This should probably be a boolean True, not \"true\"\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a one paragraph story.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "async def http_post_async(json_payload):\n",
    "\n",
    "    # Send the POST request using httpx with streaming\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        async with client.stream(\"POST\", \"http://localhost:12031/gen/v1/chat/completions\", json=json_payload) as response:\n",
    "            response.raise_for_status()\n",
    "            async for chunk in response.aiter_text():  # Use aiter_text() to handle decoding\n",
    "                chunk=json.loads(chunk)\n",
    "                chunk=chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                if chunk:  # Check for non-empty chunks\n",
    "                    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "response=await http_post_async(json_payload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-e5d82c26-bd0d-4ba0-bb03-4776abd4e500\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":[{\"id\":\"call_ea605d64-5210-4620-bdba-4194e8ceeb51\",\"function\":{\"arguments\":\"{\\\"search_query\\\": \\\"current time in Singapore\\\"}\",\"name\":\"google\"},\"type\":\"function\"}]}}],\"created\":1734485622,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":44,\"prompt_tokens\":417,\"total_tokens\":461,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"What is the current time in Singapore\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"tools\\\": [\\\n",
    "            {\\\n",
    "                \\\"type\\\": \\\"function\\\",\\\n",
    "                \\\"function\\\": {\\\n",
    "                    \\\"name\\\": \\\"google\\\",\\\n",
    "                    \\\"description\\\": \\\"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\\\",\\\n",
    "                    \\\"parameters\\\": {\\\n",
    "                        \\\"type\\\": \\\"object\\\",\\\n",
    "                        \\\"properties\\\": {\\\n",
    "                            \\\"search_query\\\": {\\\n",
    "                                \\\"type\\\": \\\"string\\\",\\\n",
    "                                \\\"description\\\": \\\"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\\\"\\\n",
    "                            }\\\n",
    "                        },\\\n",
    "                        \\\"required\\\": [\\\"search_query\\\"]\\\n",
    "                    }\\\n",
    "                }\\\n",
    "            }\\\n",
    "        ],\\\n",
    "        \\\"tool_choice\\\": \\\"required\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-159656fb-f60c-4aa8-8134-3c688fa0b6e5\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"{\\n   \\\"title\\\": \\\"Foundation\\\",\\n   \\\"summary\\\": \\\"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\\\",\\n   \\\"author\\\": \\\"Isaac Asimov\\\",\\n   \\\"published_year\\\": 1951\\n }\",\"refusal\":null,\"role\":\"assistant\",\"audio\":null,\"function_call\":null,\"tool_calls\":null}}],\"created\":1734485630,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":148,\"prompt_tokens\":252,\"total_tokens\":400,\"completion_tokens_details\":null,\"prompt_tokens_details\":null}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Foundation is a science fiction novel by American writer \\\n",
    "            Isaac Asimov. It is the first published in his Foundation Trilogy (later \\\n",
    "            expanded into the Foundation series). Foundation is a cycle of five \\\n",
    "            interrelated short stories, first published as a single book by Gnome Press \\\n",
    "            in 1951. Collectively they tell the early story of the Foundation, \\\n",
    "            an institute founded by psychohistorian Hari Seldon to preserve the best \\\n",
    "            of galactic civilization after the collapse of the Galactic Empire.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"json_schema\\\": {\\\"properties\\\": \\\n",
    "            {\\\"title\\\": \\\n",
    "                {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"summary\\\": {\\\"title\\\": \\\"Summary\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"author\\\": {\\\"title\\\": \\\"Author\\\", \\\n",
    "                    \\\"type\\\": \\\"string\\\"\\\n",
    "                }, \\\n",
    "                \\\"published_year\\\": {\\\n",
    "                    \\\"title\\\": \\\"Published Year\\\", \\\n",
    "                    \\\"type\\\": \\\"integer\\\"}}, \\\n",
    "                \\\"required\\\": [\\\n",
    "                    \\\"title\\\", \\\n",
    "                    \\\"summary\\\", \\\n",
    "                    \\\"author\\\", \\\n",
    "                    \\\"published_year\\\"\\\n",
    "                ], \\\n",
    "                \\\"title\\\": \\\"Book\\\", \\\n",
    "                \\\"type\\\": \\\"object\\\"\\\n",
    "            },\\\n",
    "        \\\"stream\\\": false }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Shut down the API Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Docker\n",
    "\n",
    "This test should **NOT** be run in devcontainer.\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **docker-compose: up**\n",
    "\n",
    "**Tests:**\n",
    "\n",
    "Repeat the API test (#)\n",
    "\n",
    "**Tear Down:**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **docker-compose: down**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    35  100    35    0     0    964      0 --:--:-- --:--:-- --:--:--  1060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":\"gai-ttt-svr-exllamav2\"}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:12031\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests:**\n",
    "\n",
    "Repeat the API test (#)\n",
    "\n",
    "**Tear Down:**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **Docker: stop**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging\n",
    "\n",
    "a) Container must be started with \"python -m debugpy --listen 0.0.0.0:5678 main.py\"\n",
    "\n",
    "b) Port 5678 must be opened.\n",
    "\n",
    "c) Click on \"Debug\" in Tool bar\n",
    "\n",
    "d) Select \"Attach\" > \"Run and Debug\"\n",
    "\n",
    "e) Add a \"breakpoint\" in the code\n",
    "\n",
    "f) Run the API test to see if it trigger the breakpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
