{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-To-Text LLM Server\n",
    "\n",
    "This repository is designed to be used with Visual Studio Code and Docker DevContainer.\n",
    "\n",
    "![dev-container](../img/dev-container.png)\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "a) Download model\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/Mistral-7B-Instruct-v0.3-exl2 \\\n",
    "    --local-dir ~/.gai/models/exllamav2-mistral7b \\\n",
    "    --local-dir-use-symlinks False\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"app_dir\":\"/home/kakkoii1337/.gai\"}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">6.17</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m6.17\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[41m\u001b[30mERROR   \u001b[0m \u001b[31mSingletonHost: Error loading generator None: Can't find /home/kakkoii1337/.gai/models/exllamav2-mistral7b\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">6.17</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m6.17\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Can't find /home/kakkoii1337/.gai/models/exllamav2-mistral7b",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m         free_mem()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# after disposal\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     free_mem()\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m free_mem()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SingletonHost\u001b[38;5;241m.\u001b[39mGetInstanceFromConfig(config) \u001b[38;5;28;01mas\u001b[39;00m host:\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m         \u001b[38;5;66;03m# after loading\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         free_mem()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/.venv/lib/python3.10/site-packages/gai/lib/server/singleton_host.py:60\u001b[0m, in \u001b[0;36mSingletonHost.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/.venv/lib/python3.10/site-packages/gai/lib/server/singleton_host.py:100\u001b[0m, in \u001b[0;36mSingletonHost.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munload()\n\u001b[1;32m     98\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingletonHost: Error loading generator \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/.venv/lib/python3.10/site-packages/gai/lib/server/singleton_host.py:93\u001b[0m, in \u001b[0;36mSingletonHost.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m class_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator \u001b[38;5;241m=\u001b[39m class_(gai_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__verbose)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_name\u001b[38;5;241m=\u001b[39mtarget_name\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/src/gai/ttt/server/gai_exllamav2.py:102\u001b[0m, in \u001b[0;36mGaiExLlamav2.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__verbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__verbose)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_cache(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__verbose)\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/.venv/lib/python3.10/site-packages/gai/lib/common/profile_function.py:54\u001b[0m, in \u001b[0;36mprofile_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m start_memory \u001b[38;5;241m=\u001b[39m start_track_cpu_memory()\n\u001b[1;32m     52\u001b[0m start_free_cuda_mem \u001b[38;5;241m=\u001b[39m start_track_cuda_memory()\n\u001b[0;32m---> 54\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# statistics after\u001b[39;00m\n\u001b[1;32m     57\u001b[0m duration \u001b[38;5;241m=\u001b[39m end_track_duration(start_duration)\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/src/gai/ttt/server/gai_exllamav2.py:66\u001b[0m, in \u001b[0;36mGaiExLlamav2.load_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m config\u001b[38;5;241m=\u001b[39mExLlamaV2Config()\n\u001b[1;32m     65\u001b[0m config\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir\n\u001b[0;32m---> 66\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m config\u001b[38;5;241m.\u001b[39mmax_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgai_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m8192\u001b[39m)\n\u001b[1;32m     68\u001b[0m config\u001b[38;5;241m.\u001b[39mno_flash_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgai_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_flash_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/workspaces/gai-ttt-svr/.venv/lib/python3.10/site-packages/exllamav2/config.py:164\u001b[0m, in \u001b[0;36mExLlamaV2Config.prepare\u001b[0;34m(self, no_tensors)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, no_tensors: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model_dir specified in ExLlamaV2Config\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# Load config.json\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Can't find /home/kakkoii1337/.gai/models/exllamav2-mistral7b"
     ]
    }
   ],
   "source": [
    "from gai.lib.server.singleton_host import SingletonHost\n",
    "from gai.lib.common.utils import free_mem\n",
    "from rich.console import Console\n",
    "console=Console()\n",
    "\n",
    "config = {\n",
    "    \"type\": \"ttt\",\n",
    "    \"generator_name\": \"exllamav2-mistral7b\",\n",
    "    \"engine\": \"gai.ttt.server.GaiExLlamaV2\",\n",
    "    \"model_path\": \"models/exllamav2-mistral7b\",\n",
    "    \"model_basename\": \"model\",\n",
    "    \"max_seq_len\": 8192,\n",
    "    \"prompt_format\": \"mistral\",\n",
    "    \"hyperparameters\": {\n",
    "        \"temperature\": 0.85,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 1000,\n",
    "    },\n",
    "    \"tool_choice\": \"auto\",\n",
    "    \"max_retries\": 5,\n",
    "    \"stop_conditions\": [\"<s>\", \"</s>\", \"user:\",\"\\n\\n\"],\n",
    "    \"no_flash_attn\":True,\n",
    "    \"seed\": None,\n",
    "    \"decode_special_tokens\": False,\n",
    "    \"module_name\": \"gai.ttt.server.gai_exllamav2\",\n",
    "    \"class_name\": \"GaiExLlamav2\",\n",
    "    \"init_args\": [],\n",
    "    \"init_kwargs\": {}\n",
    "}\n",
    "\n",
    "# check .gairc\n",
    "import os\n",
    "gairc=None\n",
    "with open(os.path.expanduser(\"~/.gairc\"),\"r\") as f:\n",
    "    gairc = f.read()\n",
    "print(gairc)\n",
    "\n",
    "# check ~/.gairc (if docker created .gairc)\n",
    "import json\n",
    "jsoned=json.loads(gairc)\n",
    "assert jsoned[\"app_dir\"]==\"/home/kakkoii1337/.gai\"\n",
    "\n",
    "# check ~/.gai (if docker created the mount point)\n",
    "assert os.path.exists(os.path.expanduser(\"~/.gai\"))\n",
    "\n",
    "# check exllamav2 (if exllamav2 is installed in the venv)\n",
    "from exllamav2 import ExLlamaV2Config\n",
    "\n",
    "# before loading\n",
    "free_mem()\n",
    "try:\n",
    "    with SingletonHost.GetInstanceFromConfig(config) as host:\n",
    "\n",
    "        # after loading\n",
    "        free_mem()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    # after disposal\n",
    "    free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Integration Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.24</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;91m1.24\u001b[0m\u001b[91m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.2447967529296875"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host = SingletonHost.GetInstanceFromConfig(config, verbose=False)\n",
    "host.load()\n",
    "generator = host.generator\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Test streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small village nestled between the mountains, lived a kind-hearted and hardworking farmer named Tomas. Despite the harsh conditions, he worked tirelessly to sustain his family and help his neighbors. One day, a great storm swept through the valley, destroying Tomas's crops and leaving him with nothing. However, the villagers rallied together, sharing their own resources to help Tomas rebuild. The storm may have taken his crops, but it couldn't break the spirit of the community that stood by him.\n"
     ]
    }
   ],
   "source": [
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=True)\n",
    "for message in response:\n",
    "    if message.choices[0].delta.content:\n",
    "        print(message.choices[0].delta.content, end=\"\", flush=True)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small village nestled between the mountains, lived a kind-hearted and resourceful young woman named Mia. She was known for her healing herbs and warm smile. One day, a traveling merchant lost his way in a storm and sought shelter in Mia's humble abode. Grateful for her hospitality, he left behind a magical gem, which granted Mia the power to heal any ailment. This gem transformed her into a renowned healer, and the villagers lived in peace and prosperity, thanks to Mia's selfless actions and the magical gem's power.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-b58099e8-b79c-466d-9956-aafb3242b648', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fc9d1739-2842-43d0-983b-8e07961dd8fc', function=Function(arguments='{\"search_query\": \"current time Singapore\"}', name='google'), type='function')]))], created=1723901723, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=21, prompt_tokens=336, total_tokens=357))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":\"What is the current time in Singapore?\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"\"}\n",
    "]\n",
    "tool_choice=\"required\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "response = host.generator.create(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=tool_choice,\n",
    "    stream=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-536f4c2b-b4aa-4a19-a20b-6477bf652d7f', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' {\\n  \"title\": \"Foundation\",\\n  \"summary\": \"Foundation is a science fiction novel by Isaac Asimov, the first published in his Foundation Trilogy. It is a cycle of five interrelated short stories that tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\",\\n  \"author\": \"Isaac Asimov\",\\n  \"published_year\": 1951\\n}', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1723901730, model='exllamav2-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=116, prompt_tokens=257, total_tokens=373))\n"
     ]
    }
   ],
   "source": [
    "# Define Schema\n",
    "from pydantic import BaseModel\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = host.generator.create(messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
    "    json_schema=Book.schema(),\n",
    "    stream=False\n",
    "    )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">1.42</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;91m1.42\u001b[0m\u001b[91m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.4186210632324219"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del host.generator.model\n",
    "del host.generator.cache\n",
    "del host.generator.tokenizer\n",
    "del host.generator\n",
    "import gc,torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. API Test\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "a) Press `F5` to start the API server.\n",
    "\n",
    "**Tests**:\n",
    "\n",
    "Run the following cells to test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Test Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-4c565ddc-ee01-4b63-bec5-fb8044de5075\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\" Once upon a time, in a land far, far away, there was a kingdom surrounded by a dense forest. The kingdom was ruled by a wise and just king, who loved his people dearly. However, the kingdom was plagued by a fearsome beast that lived deep within the forest. The beast would often come out at night and terrorize the villagers, causing them to live in constant fear\",\"refusal\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1724600640,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":86,\"prompt_tokens\":14,\"total_tokens\":100}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"tool_choice\\\": \\\"none\\\"}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Once upon a time, in a small village nestled between the mountains, lived a young girl named Mia. She was known for her kindness and her love for animals. One day, a wounded bird fell from the sky, unable to fly. Mia took it in, nursed it back to health, and released it back into the wild when it was better. This act of kindness sparked a movement in the village, inspiring others to help animals in need, and Mia became a beacon of compassion in her community"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Generate the JSON payload\n",
    "json_payload = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"stream\": \"true\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a one paragraph story.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request using httpx with streaming\n",
    "with httpx.Client() as client:\n",
    "    response = client.post(\"http://localhost:12031/gen/v1/chat/completions\", json=json_payload)\n",
    "    for line in response.iter_lines():\n",
    "        result = json.loads(line)\n",
    "        content = result[\"choices\"][0][\"delta\"][\"content\"]\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-45ce3df8-8340-40fb-8e84-eed89453daa7\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"refusal\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":[{\"id\":\"call_a67bf2e6-ff7c-42c0-9d70-e44b82eb167c\",\"function\":{\"arguments\":\"{\\\"search_query\\\": \\\"current time in Singapore\\\"}\",\"name\":\"google\"},\"type\":\"function\"}]}}],\"created\":1724600717,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":22,\"prompt_tokens\":335,\"total_tokens\":357}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"What is the current time in Singapore\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"tools\\\": [\\\n",
    "            {\\\n",
    "                \\\"type\\\": \\\"function\\\",\\\n",
    "                \\\"function\\\": {\\\n",
    "                    \\\"name\\\": \\\"google\\\",\\\n",
    "                    \\\"description\\\": \\\"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\\\",\\\n",
    "                    \\\"parameters\\\": {\\\n",
    "                        \\\"type\\\": \\\"object\\\",\\\n",
    "                        \\\"properties\\\": {\\\n",
    "                            \\\"search_query\\\": {\\\n",
    "                                \\\"type\\\": \\\"string\\\",\\\n",
    "                                \\\"description\\\": \\\"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\\\"\\\n",
    "                            }\\\n",
    "                        },\\\n",
    "                        \\\"required\\\": [\\\"search_query\\\"]\\\n",
    "                    }\\\n",
    "                }\\\n",
    "            }\\\n",
    "        ],\\\n",
    "        \\\"tool_choice\\\": \\\"required\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-8fff830c-45de-4e00-bca1-72289622ed2b\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\" {\\n  \\\"title\\\": \\\"Foundation\\\",\\n  \\\"summary\\\": \\\"Foundation is a science fiction novel by Isaac Asimov, the first published in his Foundation Trilogy. It is a cycle of five interrelated short stories that tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\\\",\\n  \\\"author\\\": \\\"Isaac Asimov\\\",\\n  \\\"published_year\\\": 1951\\n}\",\"refusal\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1724208911,\"model\":\"exllamav2-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":116,\"prompt_tokens\":253,\"total_tokens\":369}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Foundation is a science fiction novel by American writer \\\n",
    "            Isaac Asimov. It is the first published in his Foundation Trilogy (later \\\n",
    "            expanded into the Foundation series). Foundation is a cycle of five \\\n",
    "            interrelated short stories, first published as a single book by Gnome Press \\\n",
    "            in 1951. Collectively they tell the early story of the Foundation, \\\n",
    "            an institute founded by psychohistorian Hari Seldon to preserve the best \\\n",
    "            of galactic civilization after the collapse of the Galactic Empire.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"json_schema\\\": {\\\"properties\\\": \\\n",
    "            {\\\"title\\\": \\\n",
    "                {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"summary\\\": {\\\"title\\\": \\\"Summary\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"author\\\": {\\\"title\\\": \\\"Author\\\", \\\n",
    "                    \\\"type\\\": \\\"string\\\"\\\n",
    "                }, \\\n",
    "                \\\"published_year\\\": {\\\n",
    "                    \\\"title\\\": \\\"Published Year\\\", \\\n",
    "                    \\\"type\\\": \\\"integer\\\"}}, \\\n",
    "                \\\"required\\\": [\\\n",
    "                    \\\"title\\\", \\\n",
    "                    \\\"summary\\\", \\\n",
    "                    \\\"author\\\", \\\n",
    "                    \\\"published_year\\\"\\\n",
    "                ], \\\n",
    "                \\\"title\\\": \\\"Book\\\", \\\n",
    "                \\\"type\\\": \\\"object\\\"\\\n",
    "            },\\\n",
    "        \\\"tool_choice\\\": \\\"none\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Shut down the API Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Docker\n",
    "\n",
    "This test should **NOT** be run in devcontainer.\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "- Click on bottom left blue button and select **Reopen Folder in WSL**\n",
    "\n",
    "- Create and activate conda environment\n",
    "\n",
    "    ```bash\n",
    "    conda env create -f environment.yml\n",
    "    conda activate gai-ttt\n",
    "    ```\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Python: Select Interpreter**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **Docker: build**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **Docker: run**\n",
    "\n",
    "**Tests:**\n",
    "\n",
    "Repeat the API test (#)\n",
    "\n",
    "**Tear Down:**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **Docker: stop**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gai-ttt-svr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
