{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-To-Text LLM Server\n",
    "\n",
    "**important: Select venv Python Interpreter before you start**\n",
    "\n",
    "This repository is designed to be used with Visual Studio Code and Docker DevContainer.\n",
    "\n",
    "![dev-container](../img/dev-container.png)\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "a) Download model\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/dolphin-2.9.3-mistral-7B-32k-GGUF \\\n",
    "    dolphin-2.9.3-mistral-7B-32k-Q4_K_M.gguf \\\n",
    "    --revision 740ce4567b3392bd065637d2ac29127ca417cc45 \\\n",
    "    --local-dir ~/.gai/models/llamacpp-dolphin \\\n",
    "    --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```bash\n",
    "huggingface-cli download bartowski/Mistral-7B-Instruct-v0.3-GGUF \\\n",
    "    Mistral-7B-Instruct-v0.3-Q4_K_M.gguf \\\n",
    "    --revision 61fd4167fff3ab01ee1cfe0da183fa27a944db48 \\\n",
    "    --local-dir ~/.gai/models/llamacpp-mistral7b \\\n",
    "    --local-dir-use-symlinks False\n",
    "```\n",
    "\n",
    "b) Create gai.yml in ~/.gai\n",
    "\n",
    "```yaml\n",
    "generators:\n",
    "    ttt:\n",
    "        default: \"ttt-llamacpp-dolphin\"\n",
    "        configs:\n",
    "            ttt-llamacpp-dolphin:\n",
    "                type: \"ttt\"\n",
    "                engine: \"llamacpp\"\n",
    "                model: \"dolphin\"\n",
    "                name: \"ttt-llamacpp-dolphin\"\n",
    "                model_filepath: \"models/llamacpp-dolphin/dolphin-2.9.3-mistral-7B-32k-Q4_K_M.gguf\"\n",
    "                max_seq_len: 4096\n",
    "                prompt_format: \"mistral\"\n",
    "                hyperparameters:\n",
    "                    temperature: 0.85\n",
    "                    top_p: 0.8\n",
    "                    top_k: 50\n",
    "                    max_tokens: 1000\n",
    "                    tool_choice: \"auto\"\n",
    "                    max_retries: 5\n",
    "                    stop: [\"<|im_end|>\", \"</s>\", \"[/INST]\"]\n",
    "                module:\n",
    "                    name: \"gai.ttt.server.gai_llamacpp\"\n",
    "                    class: \"GaiLlamaCpp\"\n",
    "            ttt-llamacpp-mistral7b:\n",
    "                type: \"ttt\"\n",
    "                engine: \"llamacpp\"\n",
    "                model: \"mistral7b\"\n",
    "                name: \"ttt-llamacpp-mistral7b\"\n",
    "                model_filepath: \"models/llamacpp-mistral7b/Mistral-7B-Instruct-v0.3-Q4_K_M.gguf\"\n",
    "                max_seq_len: 4096\n",
    "                prompt_format: \"mistral\"\n",
    "                hyperparameters:\n",
    "                    temperature: 0.85\n",
    "                    top_p: 0.8\n",
    "                    top_k: 50\n",
    "                    max_tokens: 1000\n",
    "                    tool_choice: \"auto\"\n",
    "                    max_retries: 5\n",
    "                    stop: [\"<|im_end|>\", \"</s>\", \"[/INST]\"]\n",
    "                module:\n",
    "                    name: \"gai.ttt.server.gai_llamacpp\"\n",
    "                    class: \"GaiLlamaCpp\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"app_dir\":\"/home/kakkoii1337/.gai\"}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5.65</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m5.65\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5.28</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m5.28\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5.57</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m5.57\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check .gairc\n",
    "import os\n",
    "gairc=None\n",
    "with open(os.path.expanduser(\"~/.gairc\"),\"r\") as f:\n",
    "    gairc = f.read()\n",
    "print(gairc)\n",
    "\n",
    "# check ~/.gairc (if docker created .gairc)\n",
    "import json\n",
    "jsoned=json.loads(gairc)\n",
    "assert os.path.expanduser(jsoned[\"app_dir\"])==\"/home/kakkoii1337/.gai\"\n",
    "\n",
    "# check ~/.gai (if docker created the mount point)\n",
    "assert os.path.exists(os.path.expanduser(\"~/.gai\"))\n",
    "\n",
    "# Initiate\n",
    "from gai.lib.server.singleton_host import SingletonHost\n",
    "from gai.lib.common.utils import free_mem\n",
    "from rich.console import Console\n",
    "console=Console()\n",
    "\n",
    "from gai.ttt.server.config.ttt_config import TTTConfig\n",
    "ttt_config = TTTConfig(\n",
    "    type=\"ttt\",\n",
    "    engine=\"llamacpp\",\n",
    "    model=\"dolphin\",\n",
    "    name=\"ttt-exllamav2-dolphin\",\n",
    "    model_filepath=\"models/llamacpp-dolphin/dolphin-2.9.3-mistral-7B-32k-Q4_K_M.gguf\",\n",
    "    max_seq_len=4096,\n",
    "    prompt_format=\"mistral\",\n",
    "    hyperparameters={\n",
    "        \"temperature\": 0.85,\n",
    "        \"top_p\": 0.8,\n",
    "        \"top_k\": 50,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"tool_choice\": \"auto\",\n",
    "        \"max_retries\": 5,\n",
    "        \"stop\": [\"<|im_end|>\", \"</s>\", \"[/INST]\"],\n",
    "    },\n",
    "    module={\n",
    "        \"name\": \"gai.ttt.server.gai_llamacpp\",\n",
    "        \"class\": \"GaiLlamaCpp\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# before loading\n",
    "free_mem()\n",
    "try:\n",
    "    with SingletonHost.GetInstanceFromConfig(ttt_config) as host:\n",
    "\n",
    "        # after loading\n",
    "        free_mem()\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    # after disposal\n",
    "    free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Integration Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Startup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Free memory: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">5.27</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> GB</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Free memory: \u001b[1;92m5.27\u001b[0m\u001b[92m GB\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5.2718048095703125"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gai.lib.server.singleton_host import SingletonHost\n",
    "host = SingletonHost.GetInstanceFromConfig(ttt_config, verbose=False)\n",
    "host.load()\n",
    "generator = host.generator\n",
    "free_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Testing streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled between a dense forest and a towering mountain, there lived an old woman known for her wisdom and kindness. One day, a young traveler arrived at her door, exhausted and beaten by a recent storm. The old woman welcomed him in, tended to his wounds, and gave him a warm meal. As the traveler rested, he shared stories of the world beyond their village. The old woman listened, her eyes sparkling with curiosity and longing. When the traveler left, she stood at her door, watching him go with a mix of sadness and excitement. That night, she wrote in her journal, \"One must always be ready to step beyond the known, for it is only in the unknown that we find ourselves.\""
     ]
    }
   ],
   "source": [
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=True)\n",
    "for chunk in response:\n",
    "    if chunk:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village surrounded by lush greenery, there lived a kind-hearted old man named Samuel. Despite his age, Samuel was known for his strength and resilience, often helping his neighbors with their heavy chores. One day, a powerful storm struck the village, uprooting trees and causing havoc. Samuel, with his unwavering spirit, led the villagers in clearing the destruction, demonstrating that true power lies not in physical strength, but in the heart.\n"
     ]
    }
   ],
   "source": [
    "response = host.generator.create(\n",
    "    messages=[{\"role\":\"user\",\"content\":\"Tell me a one paragraph story\"},\n",
    "                {\"role\":\"assistant\",\"content\":\"\"}],\n",
    "    stream=False)\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional-kv ::= string [:] space additional-value \n",
      "string ::= [\"] string_103 [\"] space \n",
      "space ::= space_102 \n",
      "additional-value ::= object \n",
      "additional-kvs ::= additional-kv additional-kvs_6 \n",
      "additional-kvs_5 ::= [,] space additional-kv \n",
      "additional-kvs_6 ::= additional-kvs_5 additional-kvs_6 | \n",
      "object ::= [{] space object_97 [}] space \n",
      "array ::= [[] space array_13 []] space \n",
      "array_9 ::= value array_12 \n",
      "value ::= object | array | string | number | boolean | null \n",
      "array_11 ::= [,] space value \n",
      "array_12 ::= array_11 array_12 | \n",
      "array_13 ::= array_9 | \n",
      "boolean ::= boolean_15 space \n",
      "boolean_15 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] \n",
      "char ::= [^\"\\] | [\\] char_17 \n",
      "char_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "decimal-part ::= [0-9] decimal-part_48 \n",
      "decimal-part_19 ::= [0-9] decimal-part_47 \n",
      "decimal-part_20 ::= [0-9] decimal-part_46 \n",
      "decimal-part_21 ::= [0-9] decimal-part_45 \n",
      "decimal-part_22 ::= [0-9] decimal-part_44 \n",
      "decimal-part_23 ::= [0-9] decimal-part_43 \n",
      "decimal-part_24 ::= [0-9] decimal-part_42 \n",
      "decimal-part_25 ::= [0-9] decimal-part_41 \n",
      "decimal-part_26 ::= [0-9] decimal-part_40 \n",
      "decimal-part_27 ::= [0-9] decimal-part_39 \n",
      "decimal-part_28 ::= [0-9] decimal-part_38 \n",
      "decimal-part_29 ::= [0-9] decimal-part_37 \n",
      "decimal-part_30 ::= [0-9] decimal-part_36 \n",
      "decimal-part_31 ::= [0-9] decimal-part_35 \n",
      "decimal-part_32 ::= [0-9] decimal-part_34 \n",
      "decimal-part_33 ::= [0-9] \n",
      "decimal-part_34 ::= decimal-part_33 | \n",
      "decimal-part_35 ::= decimal-part_32 | \n",
      "decimal-part_36 ::= decimal-part_31 | \n",
      "decimal-part_37 ::= decimal-part_30 | \n",
      "decimal-part_38 ::= decimal-part_29 | \n",
      "decimal-part_39 ::= decimal-part_28 | \n",
      "decimal-part_40 ::= decimal-part_27 | \n",
      "decimal-part_41 ::= decimal-part_26 | \n",
      "decimal-part_42 ::= decimal-part_25 | \n",
      "decimal-part_43 ::= decimal-part_24 | \n",
      "decimal-part_44 ::= decimal-part_23 | \n",
      "decimal-part_45 ::= decimal-part_22 | \n",
      "decimal-part_46 ::= decimal-part_21 | \n",
      "decimal-part_47 ::= decimal-part_20 | \n",
      "decimal-part_48 ::= decimal-part_19 | \n",
      "function ::= [{] space function-name-kv [,] space function-arguments-kv [}] space \n",
      "function-name-kv ::= [\"] [n] [a] [m] [e] [\"] space [:] space string \n",
      "function-arguments-kv ::= [\"] [a] [r] [g] [u] [m] [e] [n] [t] [s] [\"] space [:] space function-arguments \n",
      "function-arguments ::= object \n",
      "function-kv ::= [\"] [f] [u] [n] [c] [t] [i] [o] [n] [\"] space [:] space function \n",
      "integral-part ::= [0-9] | [1-9] integral-part_84 \n",
      "integral-part_55 ::= [0-9] integral-part_83 \n",
      "integral-part_56 ::= [0-9] integral-part_82 \n",
      "integral-part_57 ::= [0-9] integral-part_81 \n",
      "integral-part_58 ::= [0-9] integral-part_80 \n",
      "integral-part_59 ::= [0-9] integral-part_79 \n",
      "integral-part_60 ::= [0-9] integral-part_78 \n",
      "integral-part_61 ::= [0-9] integral-part_77 \n",
      "integral-part_62 ::= [0-9] integral-part_76 \n",
      "integral-part_63 ::= [0-9] integral-part_75 \n",
      "integral-part_64 ::= [0-9] integral-part_74 \n",
      "integral-part_65 ::= [0-9] integral-part_73 \n",
      "integral-part_66 ::= [0-9] integral-part_72 \n",
      "integral-part_67 ::= [0-9] integral-part_71 \n",
      "integral-part_68 ::= [0-9] integral-part_70 \n",
      "integral-part_69 ::= [0-9] \n",
      "integral-part_70 ::= integral-part_69 | \n",
      "integral-part_71 ::= integral-part_68 | \n",
      "integral-part_72 ::= integral-part_67 | \n",
      "integral-part_73 ::= integral-part_66 | \n",
      "integral-part_74 ::= integral-part_65 | \n",
      "integral-part_75 ::= integral-part_64 | \n",
      "integral-part_76 ::= integral-part_63 | \n",
      "integral-part_77 ::= integral-part_62 | \n",
      "integral-part_78 ::= integral-part_61 | \n",
      "integral-part_79 ::= integral-part_60 | \n",
      "integral-part_80 ::= integral-part_59 | \n",
      "integral-part_81 ::= integral-part_58 | \n",
      "integral-part_82 ::= integral-part_57 | \n",
      "integral-part_83 ::= integral-part_56 | \n",
      "integral-part_84 ::= integral-part_55 | \n",
      "null ::= [n] [u] [l] [l] space \n",
      "number ::= number_87 number_90 number_93 space \n",
      "number_87 ::= number_88 integral-part \n",
      "number_88 ::= [-] | \n",
      "number_89 ::= [.] decimal-part \n",
      "number_90 ::= number_89 | \n",
      "number_91 ::= [eE] number_92 integral-part \n",
      "number_92 ::= [-+] | \n",
      "number_93 ::= number_91 | \n",
      "object_94 ::= string [:] space value object_96 \n",
      "object_95 ::= [,] space string [:] space value \n",
      "object_96 ::= object_95 object_96 | \n",
      "object_97 ::= object_94 | \n",
      "root ::= [{] space function-kv root_101 [}] space \n",
      "root_99 ::= [,] space root_100 \n",
      "root_100 ::= additional-kvs \n",
      "root_101 ::= root_99 | \n",
      "space_102 ::= [ ] | \n",
      "string_103 ::= char string_103 | \n",
      "ChatCompletion(id='chatcmpl-ebcf8247-8c34-4fc7-b5e8-a1bb972ff89a', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_fec3ae1b-bf40-42f7-894e-b0dcdac7c52d', function=Function(arguments='{\"search_query\": \"current time in Singapore\"}', name='google'), type='function')]))], created=1734423589, model='llamacpp-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=32, prompt_tokens=344, total_tokens=376, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\":\"user\",\"content\":\"What is the current time in Singapore?\"},\n",
    "    {\"role\":\"assistant\",\"content\":\"\"}\n",
    "]\n",
    "tool_choice=\"required\"\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"google\",\n",
    "            \"description\": \"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "response = host.generator.create(\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=tool_choice,\n",
    "    stream=False)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22158/2228101676.py:18: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  json_schema=Book.schema(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author-kv ::= [\"] [a] [u] [t] [h] [o] [r] [\"] space [:] space string \n",
      "space ::= space_43 \n",
      "string ::= [\"] string_44 [\"] space \n",
      "char ::= [^\"\\] | [\\] char_4 \n",
      "char_4 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "integer ::= integer_6 space \n",
      "integer_6 ::= integer_7 integral-part \n",
      "integer_7 ::= [-] | \n",
      "integral-part ::= [0-9] | [1-9] integral-part_38 \n",
      "integral-part_9 ::= [0-9] integral-part_37 \n",
      "integral-part_10 ::= [0-9] integral-part_36 \n",
      "integral-part_11 ::= [0-9] integral-part_35 \n",
      "integral-part_12 ::= [0-9] integral-part_34 \n",
      "integral-part_13 ::= [0-9] integral-part_33 \n",
      "integral-part_14 ::= [0-9] integral-part_32 \n",
      "integral-part_15 ::= [0-9] integral-part_31 \n",
      "integral-part_16 ::= [0-9] integral-part_30 \n",
      "integral-part_17 ::= [0-9] integral-part_29 \n",
      "integral-part_18 ::= [0-9] integral-part_28 \n",
      "integral-part_19 ::= [0-9] integral-part_27 \n",
      "integral-part_20 ::= [0-9] integral-part_26 \n",
      "integral-part_21 ::= [0-9] integral-part_25 \n",
      "integral-part_22 ::= [0-9] integral-part_24 \n",
      "integral-part_23 ::= [0-9] \n",
      "integral-part_24 ::= integral-part_23 | \n",
      "integral-part_25 ::= integral-part_22 | \n",
      "integral-part_26 ::= integral-part_21 | \n",
      "integral-part_27 ::= integral-part_20 | \n",
      "integral-part_28 ::= integral-part_19 | \n",
      "integral-part_29 ::= integral-part_18 | \n",
      "integral-part_30 ::= integral-part_17 | \n",
      "integral-part_31 ::= integral-part_16 | \n",
      "integral-part_32 ::= integral-part_15 | \n",
      "integral-part_33 ::= integral-part_14 | \n",
      "integral-part_34 ::= integral-part_13 | \n",
      "integral-part_35 ::= integral-part_12 | \n",
      "integral-part_36 ::= integral-part_11 | \n",
      "integral-part_37 ::= integral-part_10 | \n",
      "integral-part_38 ::= integral-part_9 | \n",
      "published-year-kv ::= [\"] [p] [u] [b] [l] [i] [s] [h] [e] [d] [_] [y] [e] [a] [r] [\"] space [:] space integer \n",
      "root ::= [{] space title-kv [,] space summary-kv [,] space author-kv [,] space published-year-kv [}] space \n",
      "title-kv ::= [\"] [t] [i] [t] [l] [e] [\"] space [:] space string \n",
      "summary-kv ::= [\"] [s] [u] [m] [m] [a] [r] [y] [\"] space [:] space string \n",
      "space_43 ::= [ ] | \n",
      "string_44 ::= char string_44 | \n",
      "ChatCompletion(id='chatcmpl-2f1e855b-7dbc-48dc-919a-94426c31854d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{ \"title\": \"Foundation\", \"summary\": \"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\", \"author\": \"Isaac Asimov\", \"published_year\": 1951 }', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1734423645, model='llamacpp-mistral7b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=147, prompt_tokens=131, total_tokens=278, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "# Define Schema\n",
    "from pydantic import BaseModel\n",
    "class Book(BaseModel):\n",
    "    title: str\n",
    "    summary: str\n",
    "    author: str\n",
    "    published_year: int\n",
    "\n",
    "text = \"\"\"Foundation is a science fiction novel by American writer\n",
    "Isaac Asimov. It is the first published in his Foundation Trilogy (later\n",
    "expanded into the Foundation series). Foundation is a cycle of five\n",
    "interrelated short stories, first published as a single book by Gnome Press\n",
    "in 1951. Collectively they tell the early story of the Foundation,\n",
    "an institute founded by psychohistorian Hari Seldon to preserve the best\n",
    "of galactic civilization after the collapse of the Galactic Empire.\n",
    "\"\"\"\n",
    "response = host.generator.create(messages=[{'role':'user','content':text},{'role':'assistant','content':''}], \n",
    "    json_schema=Book.schema(),\n",
    "    stream=False\n",
    "    )\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. API Test\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "a) Press `F5` to start the API server.\n",
    "\n",
    "b) Wait for the server to start.\n",
    "\n",
    "**Tests**:\n",
    "\n",
    "Run the following cells to test the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Test Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'curl -X POST \\\\\\n    http://localhost:12031/gen/v1/chat/completions \\\\\\n    -H \\'Content-Type: application/json\\' \\\\\\n    -s \\\\\\n    -N \\\\\\n    -d \"{\\\\\"model\\\\\":\\\\\"llamacpp-dolphin\\\\\", \\\\\\n        \\\\\"messages\\\\\": [ \\\\\\n            {\\\\\"role\\\\\": \\\\\"user\\\\\",\\\\\"content\\\\\": \\\\\"Tell me a story.\\\\\"}, \\\\\\n            {\\\\\"role\\\\\": \\\\\"assistant\\\\\",\\\\\"content\\\\\": \\\\\"\\\\\"} \\\\\\n        ],\\\\\\n        \\\\\"tool_choice\\\\\": \\\\\"none\\\\\"}\"\\n        \\n'' returned non-zero exit status 52.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcurl -X POST \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    http://localhost:12031/gen/v1/chat/completions \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    -H \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mContent-Type: application/json\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    -s \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    -N \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    -d \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllamacpp-dolphin\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: [ \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTell me a story.\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}, \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m            \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massistant\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m} \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        ],\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/magics/script.py:155\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/lib/python3.10/site-packages/IPython/core/magics/script.py:315\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'curl -X POST \\\\\\n    http://localhost:12031/gen/v1/chat/completions \\\\\\n    -H \\'Content-Type: application/json\\' \\\\\\n    -s \\\\\\n    -N \\\\\\n    -d \"{\\\\\"model\\\\\":\\\\\"llamacpp-dolphin\\\\\", \\\\\\n        \\\\\"messages\\\\\": [ \\\\\\n            {\\\\\"role\\\\\": \\\\\"user\\\\\",\\\\\"content\\\\\": \\\\\"Tell me a story.\\\\\"}, \\\\\\n            {\\\\\"role\\\\\": \\\\\"assistant\\\\\",\\\\\"content\\\\\": \\\\\"\\\\\"} \\\\\\n        ],\\\\\\n        \\\\\"tool_choice\\\\\": \\\\\"none\\\\\"}\"\\n        \\n'' returned non-zero exit status 52."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"llamacpp-dolphin\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Tell me a story.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"tool_choice\\\": \\\"none\\\"}\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Test Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled between two hills, there lived a young girl named Lily. She had a heart full of dreams and a spirit that refused to be tamed. Despite the limitations of her village, she yearned"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import httpx\n",
    "import asyncio\n",
    "from openai import ChatCompletion\n",
    "\n",
    "json_payload = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_tokens\": 50,\n",
    "    \"stream\": \"true\",  # This should probably be a boolean True, not \"true\"\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a one paragraph story.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "async def http_post_async(json_payload):\n",
    "\n",
    "    # Send the POST request using httpx with streaming\n",
    "    async with httpx.AsyncClient(timeout=30.0) as client:\n",
    "        async with client.stream(\"POST\", \"http://localhost:12031/gen/v1/chat/completions\", json=json_payload) as response:\n",
    "            response.raise_for_status()\n",
    "            async for chunk in response.aiter_text():  # Use aiter_text() to handle decoding\n",
    "                chunk=json.loads(chunk)\n",
    "                chunk=chunk[\"choices\"][0][\"delta\"][\"content\"]\n",
    "                if chunk:  # Check for non-empty chunks\n",
    "                    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "response=await http_post_async(json_payload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import httpx\n",
    "\n",
    "# Generate the JSON payload\n",
    "json_payload = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_new_tokens\": 1000,\n",
    "    \"stream\": \"true\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Tell me a one paragraph story.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Send the POST request using httpx with streaming\n",
    "with httpx.Client(timeout=30.0) as client:\n",
    "    response = client.post(\"http://localhost:12031/gen/v1/chat/completions\", json=json_payload)\n",
    "    for line in response.iter_lines():\n",
    "        result = json.loads(line)\n",
    "        content = result[\"choices\"][0][\"delta\"][\"content\"]\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Test Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-cc9d6318-d239-4686-926d-6e7049346b0f\",\"choices\":[{\"finish_reason\":\"tool_calls\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":null,\"refusal\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":[{\"id\":\"call_191e7a67-4fd1-4d9d-a136-f8c153ae8a4c\",\"function\":{\"arguments\":\"{\\\"location\\\": \\\"Singapore\\\"}\",\"name\":\"ask_time\"},\"type\":\"function\"}]}}],\"created\":1725122276,\"model\":\"llamacpp-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":26,\"prompt_tokens\":15,\"total_tokens\":41}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"What is the current time in Singapore\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"tools\\\": [\\\n",
    "            {\\\n",
    "                \\\"type\\\": \\\"function\\\",\\\n",
    "                \\\"function\\\": {\\\n",
    "                    \\\"name\\\": \\\"google\\\",\\\n",
    "                    \\\"description\\\": \\\"The 'google' function is a powerful tool that allows the AI to gather external information from the internet using Google search. It can be invoked when the AI needs to answer a question or provide information that requires up-to-date, comprehensive, and diverse sources which are not inherently known by the AI. For instance, it can be used to find current date, current news, weather updates, latest sports scores, trending topics, specific facts, or even the current date and time. The usage of this tool should be considered when the user's query implies or explicitly requests recent or wide-ranging data, or when the AI's inherent knowledge base may not have the required or most current information. The 'search_query' parameter should be a concise and accurate representation of the information needed.\\\",\\\n",
    "                    \\\"parameters\\\": {\\\n",
    "                        \\\"type\\\": \\\"object\\\",\\\n",
    "                        \\\"properties\\\": {\\\n",
    "                            \\\"search_query\\\": {\\\n",
    "                                \\\"type\\\": \\\"string\\\",\\\n",
    "                                \\\"description\\\": \\\"The search query to search google with. For example, to find the current date or time, use 'current date' or 'current time' respectively.\\\"\\\n",
    "                            }\\\n",
    "                        },\\\n",
    "                        \\\"required\\\": [\\\"search_query\\\"]\\\n",
    "                    }\\\n",
    "                }\\\n",
    "            }\\\n",
    "        ],\\\n",
    "        \\\"tool_choice\\\": \\\"required\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Test JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"chatcmpl-5184ce5e-8f31-4f6e-aee5-6f2ebd71341d\",\"choices\":[{\"finish_reason\":\"stop\",\"index\":0,\"logprobs\":null,\"message\":{\"content\":\"{ \\\"title\\\": \\\"Foundation\\\", \\\"summary\\\": \\\"Foundation is a science fiction novel by American writer Isaac Asimov. It is the first published in his Foundation Trilogy (later expanded into the Foundation series). Foundation is a cycle of five interrelated short stories, first published as a single book by Gnome Press in 1951. Collectively they tell the early story of the Foundation, an institute founded by psychohistorian Hari Seldon to preserve the best of galactic civilization after the collapse of the Galactic Empire.\\\", \\\"author\\\": \\\"Isaac Asimov\\\", \\\"published_year\\\": 1951}\",\"refusal\":null,\"role\":\"assistant\",\"function_call\":null,\"tool_calls\":null}}],\"created\":1725122322,\"model\":\"llamacpp-mistral7b\",\"object\":\"chat.completion\",\"service_tier\":null,\"system_fingerprint\":null,\"usage\":{\"completion_tokens\":146,\"prompt_tokens\":120,\"total_tokens\":266}}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -X POST \\\n",
    "    http://localhost:12031/gen/v1/chat/completions \\\n",
    "    -H 'Content-Type: application/json' \\\n",
    "    -s \\\n",
    "    -N \\\n",
    "    -d \"{\\\"model\\\":\\\"exllamav2-mistral7b\\\", \\\n",
    "        \\\"messages\\\": [ \\\n",
    "            {\\\"role\\\": \\\"user\\\",\\\"content\\\": \\\"Foundation is a science fiction novel by American writer \\\n",
    "            Isaac Asimov. It is the first published in his Foundation Trilogy (later \\\n",
    "            expanded into the Foundation series). Foundation is a cycle of five \\\n",
    "            interrelated short stories, first published as a single book by Gnome Press \\\n",
    "            in 1951. Collectively they tell the early story of the Foundation, \\\n",
    "            an institute founded by psychohistorian Hari Seldon to preserve the best \\\n",
    "            of galactic civilization after the collapse of the Galactic Empire.\\\"}, \\\n",
    "            {\\\"role\\\": \\\"assistant\\\",\\\"content\\\": \\\"\\\"} \\\n",
    "        ],\\\n",
    "        \\\"json_schema\\\": {\\\"properties\\\": \\\n",
    "            {\\\"title\\\": \\\n",
    "                {\\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"summary\\\": {\\\"title\\\": \\\"Summary\\\", \\\"type\\\": \\\"string\\\"}, \\\n",
    "                    \\\"author\\\": {\\\"title\\\": \\\"Author\\\", \\\n",
    "                    \\\"type\\\": \\\"string\\\"\\\n",
    "                }, \\\n",
    "                \\\"published_year\\\": {\\\n",
    "                    \\\"title\\\": \\\"Published Year\\\", \\\n",
    "                    \\\"type\\\": \\\"integer\\\"}}, \\\n",
    "                \\\"required\\\": [\\\n",
    "                    \\\"title\\\", \\\n",
    "                    \\\"summary\\\", \\\n",
    "                    \\\"author\\\", \\\n",
    "                    \\\"published_year\\\"\\\n",
    "                ], \\\n",
    "                \\\"title\\\": \\\"Book\\\", \\\n",
    "                \\\"type\\\": \\\"object\\\"\\\n",
    "            },\\\n",
    "        \\\"tool_choice\\\": \\\"none\\\"}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Shut down the API Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Docker\n",
    "\n",
    "**Instructions:** \n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **docker-compose: up**\n",
    "\n",
    "#### Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    34  100    34    0     0   1014      0 --:--:-- --:--:-- --:--:--  1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"message\":\"gai-ttt-svr-llamacpp\"}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl http://localhost:12031\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests:**\n",
    "\n",
    "Repeat the API test (#)\n",
    "\n",
    "**Tear Down:**\n",
    "\n",
    "- Press **CTRL+SHIFT+P** > **Tasks: Run Task** > **docker-compose: down**\n",
    "\n",
    "### Debugging\n",
    "\n",
    "a) Container must be started with \"python -m debugpy --listen 0.0.0.0:5678 main.py\"\n",
    "\n",
    "b) Port 5678 must be opened.\n",
    "\n",
    "c) Click on \"Debug\" in Tool bar\n",
    "\n",
    "d) Select \"Attach\" > \"Run and Debug\"\n",
    "\n",
    "e) Add a \"breakpoint\" in the code\n",
    "\n",
    "f) Run the API test to see if it trigger the breakpoint."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
